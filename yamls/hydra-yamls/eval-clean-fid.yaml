image_size: 256 # This is the image resolution to evaluate at (assumes square images)
batch_size: 16
name: clean-fid-eval # Name for the eval run for logging
project: diffusion-clean-fid-eval # Name of the wandb project for logging
seed: 42 # Random seed. This affects the randomness used in image generation.

model: # This is the model to evaluate
  _target_: diffusion.models.models.stable_diffusion_2
  pretrained: false
  precomputed_latents: false
  encode_latents_in_fp16: true
  fsdp: false
eval_dataloader:
  _target_: diffusion.datasets.build_streaming_image_caption_dataloader
  remote:
    - # Remote(s) for the evaluation dataset go here
  local:
    - # Local(s) for the evaluation dataset go here
  batch_size: ${batch_size}
  resize_size: ${image_size}
  image_key: image # This should be set to the image key specific to the eval dataset
  caption_key: captions # This should be set to the caption key specific to the eval dataset
  transform: # How to transform the images for evaluation
    - _target_ : diffusion.datasets.laion.transforms.LargestCenterSquare
      size: ${image_size}
    - _target_: torchvision.transforms.ToTensor
  dataloader_kwargs:
    drop_last: false
    shuffle: false
    num_workers: 8
    pin_memory: true
  streaming_kwargs:
    shuffle: false
clip_metric: # This is the metric used to compute CLIP score, which is not part of clean-fid
  _target_: torchmetrics.multimodal.CLIPScore
  model_name_or_path: openai/clip-vit-base-patch16
logger:
  wandb:
    _target_: composer.loggers.wandb_logger.WandBLogger
    name: ${name}
    project: ${project}
    group: ${name}
evaluator:
  _target_: diffusion.evaluation.clean_fid_eval.CleanFIDEvaluator
  load_path: # Path to the checkpoint to load and evaluate.
  guidance_scales:
    - 1.0
    - 1.5
    - 2.0
    - 3.0
    - 4.0
    - 5.0
    - 6.0
    - 7.0
    - 8.0
  size: ${image_size}
  batch_size: ${batch_size}
  seed: ${seed}
